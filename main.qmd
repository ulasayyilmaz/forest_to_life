---
title: "Deforestation-to-biodiversity"
author: "Ulas Ayyilmaz and Ishika"
format: pdf
execute:
  warning: false
  message: false
---

```{r}
#| echo: false
library(tidyverse)
library(ISLR)
library(tidymodels)
library(dplyr)
library("rgbif")
library(caret)
library(randomForest)
library(neuralnet)
library(readr)
```

```{r}
#change between 2000-2020 #https://www.globalforestwatch.org/dashboards/global/?category=forest-change&location=WyJnbG9iYWwiXQ%3D%3D&scrollTo=net-change
forest_data <- read_csv("net_tree_change.csv")
head(forest_data)

```

```{r}
top_5_loss <- forest_data |>
  arrange(desc(as.numeric(loss))) |>
  slice_head(n = 20)
top_5_gain <- forest_data |>
  arrange(desc(as.numeric(gain))) |>
  slice_head(n = 20)

top_5_net_desc <- forest_data |>
  arrange(desc(as.numeric(net))) |>
  slice_head(n = 20)

top_5_net_asc <- forest_data |>
  arrange(as.numeric(net)) |>
  slice_head(n = 20)

top_5_net_desc # poland, ukraine, uruguay, ireland, bangladesh
top_5_net_asc#tanzania, Mozambique, indonesia, DCcongo, paraguay
```
```{r}

```


order of interest (check exist for each country)

Most net negative:
PRY, COD, MOZ, IDN, TZA

most positive:
URY, UKR, POL,IRL, BGD

All columns of a RGBIF EOD dataset
#"gbifID", "datasetKey", "occurrenceID", "kingdom", "phylum", "class", "order", "family", #"genus", "species", "infraspecificEpithet", "taxonRank", "scientificName", #"verbatimScientificName", "verbatimScientificNameAuthorship", "countryCode", "locality", #"stateProvince", "occurrenceStatus", "individualCount", "publishingOrgKey", #"decimalLatitude", "decimalLongitude", "coordinateUncertaintyInMeters", #"coordinatePrecision", "elevation", "elevationAccuracy", "depth", "depthAccuracy", #"eventDate", "day", "month", "year", "taxonKey", "speciesKey", "basisOfRecord", #"institutionCode", "collectionCode", "catalogNumber", "recordNumber", "identifiedBy", #"dateIdentified", "license", "rightsHolder", "recordedBy", "typeStatus", #"establishmentMeans", "lastInterpreted", "mediaType", "issue")


Ok, brainstorm time. Deforestation means removel of forests over time due to mostly due to human interference. Deforestation affects many things that contribute to human's well-being indirectly in a negative way. A direct effect is observed on the biodiversity - specifically birds who roam freely in forests. Other 

```{r}
bird_data<-name_suggest(q = "Aves", rank = "class", curlopts = list(timeout = 60))
bird_taxon_key <- bird_data$data$key
eod_dataset_key <- "4fa7b334-ce0d-4e88-aaae-2e0c138d049e"  # EOD datasetKey
```

```{r}
# Define the target countries and years
years <- c(2000, 2010, 2020)  # Year range
eod_dataset_key <- "4fa7b334-ce0d-4e88-aaae-2e0c138d049e"  # EOD datasetKey
orders <- c("Coraciiformes","Strigiformes","Galliformes","Ciconiiformes")
```



```{r}
# Load necessary library

# Initialize an empty dataframe to store the combined data
combined_data <- data.frame()

# List of country names (folders inside the "data" directory)
countries <- c( "poland","ukraine", "uruguay", "ireland", "bangladesh",
               "tanzania", "mozambique", "indonesia", "dcongo", "paraguay")
# Loop through each country's folder and combine all CSV files
for (country in countries) {
  
  # Path to the country's folder
  country_path <- file.path("data", country)

    # Get the list of CSV files in the country's folder
  csv_files <- list.files(country_path, pattern = "\\.csv$", full.names = TRUE)

    # Read each CSV file and add its content to the combined dataframe
  for (csv_file in csv_files) {
   tryCatch({
      # Read the CSV file (use read_delim for robustness)
      data <- read_delim(csv_file, delim = NULL, show_col_types = FALSE)
      
      # Add a column to identify the country
      data$country <- country
      
      # Append the data to the combined dataframe
      combined_data <- bind_rows(combined_data, data)
    }, error = function(e) {
      cat("Error reading file:", csv_file, "\n")
    })
  }
}

# Save the combined dataframe as a CSV file in the main directory
write.csv(combined_data, "all_bird_orders_data.csv", row.names = FALSE)

cat("Combined CSV file created as 'all_bird_orders_data.csv' in the main directory.\n")

```

```{r}
bird_orders <- read.csv("all_bird_orders_data.csv")
head(bird_orders)
#596024662,584212444
```
info: all belong to aves class
#important columns to keep:
order, family, genus, species, stateProvince, individualCount, decimalLatitude, elevation,decimalLongitude, day,month, year, country

```{r}
filtered_bird_orders <- bird_orders |>
  filter(order %in% orders)|> 
  select(order, family, genus, species, stateProvince, individualCount, decimalLatitude, elevation,decimalLongitude, day,month, year, country)
head(filtered_bird_orders)
```

```{r}
country_mapping <- data.frame(
  iso = c("POL", "UKR", "URY", "IRL", "BGD", "TZA", "MOZ", "IDN", "COD", "PRY"),
  country = c("poland", "ukraine", "uruguay", "ireland", "bangladesh",
              "tanzania", "mozambique", "indonesia", "dcongo", "paraguay")
)

filtered_bird_orders$country <- tolower(filtered_bird_orders$country)

filtered_forest_data <- forest_data %>%
  filter(iso %in% country_mapping$iso) %>%   # Keep only rows with matching ISO codes
  left_join(country_mapping, by = "iso")
```

```{r}
forest_change <- filtered_forest_data |> 
  select(country,gain,loss,gfw_area__ha )
```


```{r}
# Combine the datasets based on the "country" column
combined_data <- left_join(filtered_bird_orders, filtered_forest_data, by = "country")

# Output the combined dataset
write.csv(combined_data, "filtered_forest_data.csv")
```

```{r}
joined_data <- read.csv("filtered_forest_data.csv")
joined_data
```



```{r}
#get rid of NA's in joined data
joined_data1 <- joined_data |>
  filter(!is.na(individualCount))

country_density_map <- joined_data1 |> 
  group_by(country,order,year)

country_density_map

```

```{r}
library(sf)
library(tidyr)
library(rnaturalearth)
```

```{r}
# # Determine the global range of individualCount for consistent scaling
# global_size_range <- range(country_density_map$individualCount, na.rm = TRUE)
# 
# # Iterate through each bird order and create maps for each country and year
# bird_orders <- unique(country_density_map$order)
# 
# # Create a list to store plots
# plots <- list()
# c = 0
# for (order1 in bird_orders) {
#   for (country1 in countries) {
#     for (year1 in years) {
#       c <- c + 1
#       # Filter data for the specific bird order, year, and country
#       specific_data <- country_density_map |>
#         filter(order == order1, year == year1, country == country1)
#       
#       # Join country map with bird observation data
#       country_map <- country_maps |> filter(name_long == country1)
# 
#       # Create the plot
#       plots[[c]] <-
#         ggplot() +
#         geom_sf(data = country_map, fill = "gray90", color = "black") +
#         geom_point(data = specific_data, 
#                    aes(x = decimalLongitude, y = decimalLatitude, size = individualCount),
#                    alpha = 0.7) +
#         scale_size_continuous(name = "Count", range = c(1, 10), limits = global_size_range) +
#         ggtitle(paste("Bird Order:", order1, "Year:", year1, "Country:", country1)) +
#         theme_bw()
#       
#       ggsave(paste("plot_geodistribution/bird_order:", order1, "year:", year1, "country:", country1,".png"), plots[[c]], width = 10, height = 7)
# 
#     }
#   }
# }

```


```{r}
# ggsave("bird_density_map.png", plots[[1]], width = 10, height = 7)
# Example: Display the first plot
print(length(plots))
print(plots[[1]])
print(plots[[2]])
print(plots[[3]])

```


```{r}
# Calculate the global maximum change
max_change <- filtered_forest_data |> 
  mutate(
    initial_forest = as.numeric(gfw_area__ha),
    final_forest = as.numeric(gfw_area__ha) - as.numeric(loss) + as.numeric(gain),
    change = final_forest - initial_forest
  ) |> 
  summarise(max_change = max(abs(change), na.rm = TRUE)) |> 
  pull(max_change)

forest_change <- filtered_forest_data |> 
  mutate(
    initial_forest = as.numeric(gfw_area__ha),
    final_forest = as.numeric(gfw_area__ha) - as.numeric(loss) + as.numeric(gain),
    change = final_forest - initial_forest,
    color_intensity = abs(change) / max_change,
    line_color = ifelse(change > 0, 
                        scales::col_numeric("green", domain = c(0, 1))(color_intensity), 
                        scales::col_numeric("red", domain = c(0, 1))(color_intensity))
  ) |> 
  select(country, initial_forest, final_forest, change, line_color) |>
  pivot_longer(cols = c(initial_forest, final_forest), 
               names_to = "year", 
               values_to = "forest_area") |>
  mutate(year = recode(year, 
                       initial_forest = "2000", 
                       final_forest = "2020"))

# Create a single ggplot for forest change across 10 countries
forest_change_plot <- ggplot(forest_change, aes(x = year, y = forest_area, group = country)) +
  geom_line(aes(color = line_color), size = 1.2) +
  geom_point(aes(color = line_color), size = 3) +
  scale_color_identity() +
  geom_text(data = forest_change |> filter(year == "2020"), 
            aes(label = country, x = year, y = forest_area, color = line_color),
            hjust = -0.2, size = 3) +
  ggtitle("Forest Area Change (2000 vs 2020) Across Countries") +
  xlab("Year") +
  ylab("Forest Area (ha)") +
  theme_minimal()

# Display the plot
print(forest_change_plot)


```


```{r}
print(length(plots))
```




```{r}
# Aggregate bird count by order, country, and year
order_counts <- joined_data %>%
  group_by(order, country, year) %>%
  summarize(order_count = sum(individualCount, na.rm = TRUE), .groups = "drop")

order_counts
```

```{r}
# #visualize geographical distribution with log normalized data
# country_density_map_logscale <- country_density_map|>
#   mutate(
#     log_count = log1p(individualCount)  # log1p handles zero counts
#   )
# 
# global_size_range <- range(country_density_map_logscale$log_count, na.rm = TRUE)
# 
# # Iterate through each bird order and create maps for each country and year
# bird_orders <- unique(country_density_map_logscale$order)
# 
# # Create a list to store plots
# plots <- list()
# c = 0
# for (order1 in bird_orders) {
#   for (country1 in countries) {
#     for (year1 in years) {
#       c <- c + 1
#       # Filter data for the specific bird order, year, and country
#       specific_data <- country_density_map_logscale |>
#         filter(order == order1, year == year1, country == country1)
#       
#       # Join country map with bird observation data
#       country_map <- country_maps |> filter(name_long == country1)
# 
#       # Create the plot
#       plots[[c]] <-
#         ggplot() +
#         geom_sf(data = country_map, fill = "gray90", color = "black") +
#         geom_point(data = specific_data, 
#                    aes(x = decimalLongitude, y = decimalLatitude, size = individualCount),
#                    alpha = 0.7) +
#         scale_size_continuous(name = "Count", range = c(1, 10), limits = global_size_range) +
#         ggtitle(paste("Bird Order:", order1, "Year:", year1, "Country:", country1)) +
#         theme_bw()
#       
#       ggsave(paste("plot_geodistribution_logscale/bird_order:", order1, "year:", year1, "country:", country1,".png"), plots[[c]], width = 10, height = 7)
# 
#     }
#   }
# }

```


```{r}
# Merge aggregated counts back with deforestation data
model_data <- joined_data1 %>%
  select(country, year, iso, stable, loss, gain, disturb, net, change, gfw_area__ha) %>%
  distinct() %>%
  inner_join(order_counts, by = c("country", "year"))

#log normalizes the counts to take into account discrepancies in effort put into observation between 2000,2010  and 2020
model_data_log <- model_data|>
  mutate(
    log_count = log1p(order_count)  # log1p handles zero counts
  )
model_data_log
```

```{r}
model_data_log_normalized <- model_data_log |> 
  mutate(
    scaled_log_count = (log_count - min(log_count, na.rm = TRUE)) / 
                       (max(log_count, na.rm = TRUE) - min(log_count, na.rm = TRUE))
  )

model_data_log_normalized
```

```{r}
model_data_log_normalized_bins <- model_data_log_normalized |>
  mutate(
    scaled_log_count_bins = cut(
      scaled_log_count,
      breaks = seq(0, 1, length.out = 11),  # 10 intervals
      labels = paste0("b", 1:10),       # Create labels for the bins
      include.lowest = TRUE
    )
  )
model_data_log_normalized_bins
```


```{r}
set.seed(123)  # For reproducibility
library(caret)

# Perform an initial split
split_index <- initial_split(model_data_log_normalized_bins, prop = 0.8)

# Extract training and test sets
train_data <- training(split_index)
test_data <- testing(split_index)

```

```{r}
library(e1071)

svm_recipe <-
  recipe(scaled_log_count_bins ~ stable + loss + gain + net + change + gfw_area__ha + order + year + country, 
         data = model_data_log_normalized_bins) |>
  step_mutate(country = as.factor(country)) |>
  step_dummy(country, one_hot = TRUE) |>
  step_mutate(order = as.factor(order)) |>
  step_dummy(order, one_hot = TRUE) |>
  step_mutate(year = factor(year, levels = c(2000, 2010, 2020)))


svm_pol <- svm_poly(cost = tune(), degree= tune()) |>
  set_engine("kernlab") |>
  set_mode("classification")

svm_pol_wflow <- workflow() |>
  add_model(svm_pol) |>
  add_recipe(svm_recipe)

folds_pol <- vfold_cv(train_data, v = 4)

# the tuned parameters also have default values you can use
grid_pol <- grid_regular(cost(), degree(c(1,5)), levels = 5)

svm_pol_tune <- 
  svm_pol_wflow |>
  tune_grid(resamples = folds_pol, grid = grid_pol)


svm_metrics_pol <- collect_metrics(svm_pol_tune)
accuracy_results_pol <- svm_metrics_pol |> 
  filter(.metric == "accuracy")
print(accuracy_results_pol)

```


```{r}
library(randomForest)
 rf_recipe <-
  recipe(scaled_log_count_bins ~ stable + loss + gain + net + change + gfw_area__ha + order + year + country, 
         data = model_data_log_normalized_bins)

carseats_rf <- rand_forest(mtry = 10) |>
  set_engine("ranger", importance="permutation") |>
  set_mode("regression")

carseats_rf_wflow <- workflow() |>
  add_model(carseats_rf) |>
  add_recipe(carseats_recipe)

carseats_rf_fit <- 
  carseats_rf_wflow |>
  fit(data = carseats_train)

carseats_rf_preds <- predict(carseats_rf_fit, new_data = carseats_test) |>
  bind_cols(carseats_test)

# Step 4: Calculate MSE for each model
carseats_rf_mse <- carseats_rf_preds |>
  metrics(truth = Sales, estimate = .pred) |>
  filter(.metric == "rmse") |>
  pull(.estimate)

carseats_rf_mse



# Random forest model with tuning
rf_model <- train(
  order_count ~ stable + loss + gain + disturb + net + change + gfw_area__ha,
  data = train_data,
  method = "rf",
  trControl = trainControl(method = "oob", search = "grid"),
  tuneGrid = expand.grid(mtry = 2:6)  # Test different values of mtry
)

# Print results
print(rf_model)

# Visualize the trained random forest model
plot(rf_model)

```

```{r}
#calculate prediction accuracy for train and test data with the trained random forest

```




```{r}
#calculate prediction accuracy
```


```{r}
# KNN model with tuning
knn_model <- train(
  order_count ~ stable + loss + gain + disturb + net + change + gfw_area__ha,
  data = train_data,
  method = "knn",
  trControl = trainControl(method = "cv", number = 10),  # 10-fold cross-validation
  tuneGrid = expand.grid(k = seq(1, 15, by = 2))  # Test different values of k
)

# Print results
print(knn_model)

# Visualize the KNN model performance
plot(knn_model)

```

```{r}
# Make predictions
rf_preds <- predict(rf_model, test_data)
knn_preds <- predict(knn_model, test_data)
svm_preds <- predict(svm_model, test_data)

# Evaluate performance (example: RMSE)
rf_rmse <- RMSE(rf_preds, test_data$order_count)
knn_rmse <- RMSE(knn_preds, test_data$order_count)
svm_rmse <- RMSE(svm_preds, test_data$order_count)

cat("Random Forest RMSE:", rf_rmse, "\n")
cat("KNN RMSE:", knn_rmse, "\n")
cat("SVM RMSE:", svm_rmse, "\n")
```

```{r}
# Variable importance plot
varImpPlot(rf_model$finalModel)
```
```{r}
# Predict order_count using each model
rf_preds <- predict(rf_model, test_data)  # Random Forest predictions
knn_preds <- predict(knn_model, test_data)  # KNN predictions
svm_preds <- predict(svm_model, test_data)  # SVM predictions

# Load necessary library for evaluation metrics
library(Metrics)

# Define metrics
calc_metrics <- function(actual, predicted) {
  r_squared <- cor(actual, predicted)^2
  rmse_val <- rmse(actual, predicted)
  mae_val <- mae(actual, predicted)
  
  return(data.frame(
    R_Squared = r_squared,
    RMSE = rmse_val,
    MAE = mae_val
  ))
}

# Calculate metrics for each model
rf_metrics <- calc_metrics(test_data$order_count, rf_preds)
knn_metrics <- calc_metrics(test_data$order_count, knn_preds)
svm_metrics <- calc_metrics(test_data$order_count, svm_preds)

# Combine results for comparison
model_metrics <- rbind(
  cbind(Model = "Random Forest", rf_metrics),
  cbind(Model = "KNN", knn_metrics),
  cbind(Model = "SVM", svm_metrics)
)

print(model_metrics)


```
```{r}
# Combine actual and predicted data for visualization
test_results <- test_data %>%
  mutate(
    RF_Preds = rf_preds,
    KNN_Preds = knn_preds,
    SVM_Preds = svm_preds
  )

# Create scatter plots for each model
library(ggplot2)

ggplot(test_results, aes(x = order_count, y = RF_Preds)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Random Forest Predictions vs Actual",
    x = "Actual Order Count",
    y = "Predicted Order Count"
  ) +
  theme_minimal()

ggplot(test_results, aes(x = order_count, y = KNN_Preds)) +
  geom_point(color = "green") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "KNN Predictions vs Actual",
    x = "Actual Order Count",
    y = "Predicted Order Count"
  ) +
  theme_minimal()

ggplot(test_results, aes(x = order_count, y = SVM_Preds)) +
  geom_point(color = "purple") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "SVM Predictions vs Actual",
    x = "Actual Order Count",
    y = "Predicted Order Count"
  ) +
  theme_minimal()

```

