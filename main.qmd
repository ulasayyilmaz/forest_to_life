---
title: "Deforestation-to-biodiversity"
author: "Ulas Ayyilmaz and Ishika"
format: pdf
execute:
  warning: false
  message: false
---

```{r}
#| echo: false
library(tidyverse)
library(ISLR)
library(tidymodels)
library(dplyr)
library("rgbif")
library(caret)
library(randomForest)
library(neuralnet)
library(readr)
```

```{r}
#change between 2000-2020 #https://www.globalforestwatch.org/dashboards/global/?category=forest-change&location=WyJnbG9iYWwiXQ%3D%3D&scrollTo=net-change
forest_data <- read_csv("net_tree_change.csv")
head(forest_data)

```

```{r}
top_5_loss <- forest_data |>
  arrange(desc(as.numeric(loss))) |>
  slice_head(n = 20)
top_5_gain <- forest_data |>
  arrange(desc(as.numeric(gain))) |>
  slice_head(n = 20)

top_5_net_desc <- forest_data |>
  arrange(desc(as.numeric(net))) |>
  slice_head(n = 20)

top_5_net_asc <- forest_data |>
  arrange(as.numeric(net)) |>
  slice_head(n = 20)

top_5_net_desc # poland, ukraine, uruguay, ireland, bangladesh
top_5_net_asc#tanzania, Mozambique, indonesia, DCcongo, paraguay
```


order of interest (check exist for each country)

Most net negative:
PRY, COD, MOZ, IDN, TZA

most positive:
URY, UKR, POL,IRL, BGD

All columns of a RGBIF EOD dataset
#"gbifID", "datasetKey", "occurrenceID", "kingdom", "phylum", "class", "order", "family", #"genus", "species", "infraspecificEpithet", "taxonRank", "scientificName", #"verbatimScientificName", "verbatimScientificNameAuthorship", "countryCode", "locality", #"stateProvince", "occurrenceStatus", "individualCount", "publishingOrgKey", #"decimalLatitude", "decimalLongitude", "coordinateUncertaintyInMeters", #"coordinatePrecision", "elevation", "elevationAccuracy", "depth", "depthAccuracy", #"eventDate", "day", "month", "year", "taxonKey", "speciesKey", "basisOfRecord", #"institutionCode", "collectionCode", "catalogNumber", "recordNumber", "identifiedBy", #"dateIdentified", "license", "rightsHolder", "recordedBy", "typeStatus", #"establishmentMeans", "lastInterpreted", "mediaType", "issue")


Ok, brainstorm time. Deforestation means removel of forests over time due to mostly due to human interference. Deforestation affects many things that contribute to human's well-being indirectly in a negative way. A direct effect is observed on the biodiversity - specifically birds who roam freely in forests. Other 

```{r}
bird_data<-name_suggest(q = "Aves", rank = "class", curlopts = list(timeout = 60))
bird_taxon_key <- bird_data$data$key
eod_dataset_key <- "4fa7b334-ce0d-4e88-aaae-2e0c138d049e"  # EOD datasetKey
```

```{r}
# Define the target countries and years
years <- c(2000, 2010, 2020)  # Year range
eod_dataset_key <- "4fa7b334-ce0d-4e88-aaae-2e0c138d049e"  # EOD datasetKey
orders <- c("Coraciiformes","Strigiformes","Galliformes","Ciconiiformes")
```



```{r}
# Load necessary library
# Initialize an empty dataframe to store the combined data
combined_data <- data.frame()

# List of country names (folders inside the "data" directory)
countries <- c( "poland","ukraine", "uruguay", "ireland", "bangladesh",
               "tanzania", "mozambique", "indonesia", "dcongo", "paraguay")
# Loop through each country's folder and combine all CSV files
for (country in countries) {
  
  # Path to the country's folder
  country_path <- file.path("data", country)

    # Get the list of CSV files in the country's folder
  csv_files <- list.files(country_path, pattern = "\\.csv$", full.names = TRUE)

    # Read each CSV file and add its content to the combined dataframe
  for (csv_file in csv_files) {
   tryCatch({
      # Read the CSV file (use read_delim for robustness)
      data <- read_delim(csv_file, delim = NULL, show_col_types = FALSE)
      
      # Add a column to identify the country
      data$country <- country
      
      # Append the data to the combined dataframe
      combined_data <- bind_rows(combined_data, data)
    }, error = function(e) {
      cat("Error reading file:", csv_file, "\n")
    })
  }
}

# Save the combined dataframe as a CSV file in the main directory
write.csv(combined_data, "all_bird_orders_data.csv", row.names = FALSE)

cat("Combined CSV file created as 'all_bird_orders_data.csv' in the main directory.\n")

```

```{r}
bird_orders <- read.csv("all_bird_orders_data.csv")
head(bird_orders)
#596024662,584212444
```
info: all belong to aves class
#important columns to keep:
order, family, genus, species, stateProvince, individualCount, decimalLatitude, elevation,decimalLongitude, day,month, year, country

```{r}
filtered_bird_orders <- bird_orders |>
  filter(order %in% orders)|> 
  select(order, family, genus, species, stateProvince, individualCount, decimalLatitude, elevation,decimalLongitude, day,month, year, country)
head(filtered_bird_orders)
```

```{r}
country_mapping <- data.frame(
  iso = c("POL", "UKR", "URY", "IRL", "BGD", "TZA", "MOZ", "IDN", "COD", "PRY"),
  country = c("poland", "ukraine", "uruguay", "ireland", "bangladesh",
              "tanzania", "mozambique", "indonesia", "dcongo", "paraguay")
)

filtered_forest_data <- forest_data %>%
  filter(iso %in% country_mapping$iso) %>%   # Keep only rows with matching ISO codes
  left_join(country_mapping, by = "iso")
```


```{r}
# Combine the datasets based on the "country" column
joined_data <- left_join(filtered_bird_orders, filtered_forest_data, by = "country")

# Output the combined dataset
write.csv(joined_data, "filtered_forest_data.csv")
joined_data

#get rid of NA's in joined data
joined_data1 <- joined_data |>
  filter(!is.na(individualCount))
```

```{r}
library(sf)
library(tidyr)
library(rnaturalearth)
```

```{r}
order_counts <- joined_data1 %>%
  group_by(order, country, year) %>%
  summarize(order_count = sum(individualCount), .groups = "drop")
library(dplyr)
```

```{r}
country_density_map <- joined_data1 |> 
  group_by(country,order,year)

# Determine the global range of individualCount for consistent scaling
global_size_range <- range(country_density_map$individualCount, na.rm = TRUE)

# Iterate through each bird order and create maps for each country and year
bird_orders <- unique(country_density_map$order)

# Create a list to store plots
plots <- list()
c = 0
world_map <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")
country_maps <- world_map |> filter(name_long %in% countries)

for (order1 in bird_orders) {
  for (country1 in countries) {
    for (year1 in years) {
      c <- c + 1
      # Filter data for the specific bird order, year, and country
      specific_data <- country_density_map |>
        filter(order == order1, year == year1, country == country1)

      # Join country map with bird observation data
      country_map <- country_maps |> filter(name_long == country1)

      # Create the plot
      plots[[c]] <-
        ggplot() +
        geom_sf(data = country_map, fill = "gray90", color = "black") +
        geom_point(data = specific_data,
                   aes(x = decimalLongitude, y = decimalLatitude, size = individualCount),
                   alpha = 0.7) +
        scale_size_continuous(name = "Count", range = c(1, 10), limits = global_size_range) +
        ggtitle(paste("Bird Order:", order1, "Year:", year1, "Country:", country1)) +
        theme_bw()

      ggsave(paste("plot_geodistribution/bird_order:", order1, "year:", year1, "country:", country1,".png"), plots[[c]], width = 10, height = 7)

    }
  }
}

```


```{r}
# ggsave("bird_density_map.png", plots[[1]], width = 10, height = 7)
# Example: Display the first plot
print(length(plots))
print(plots[[1]])
print(plots[[2]])
print(plots[[3]])

```


```{r}
# Calculate the global maximum change
max_change <- filtered_forest_data |> 
  mutate(
    initial_forest = as.numeric(gfw_area__ha),
    final_forest = as.numeric(gfw_area__ha) - as.numeric(loss) + as.numeric(gain),
    change = final_forest - initial_forest
  ) |> 
  summarise(max_change = max(abs(change), na.rm = TRUE)) |> 
  pull(max_change)

forest_change <- filtered_forest_data |> 
  mutate(
    initial_forest = as.numeric(gfw_area__ha),
    final_forest = as.numeric(gfw_area__ha) - as.numeric(loss) + as.numeric(gain),
    change = final_forest - initial_forest,
    color_intensity = abs(change) / max_change,
    line_color = ifelse(change > 0, 
                        scales::col_numeric("green", domain = c(0, 1))(color_intensity), 
                        scales::col_numeric("red", domain = c(0, 1))(color_intensity))
  ) |> 
  select(country, initial_forest, final_forest, change, line_color) |>
  pivot_longer(cols = c(initial_forest, final_forest), 
               names_to = "year", 
               values_to = "forest_area") |>
  mutate(year = recode(year, 
                       initial_forest = "2000", 
                       final_forest = "2020"))

# Create a single ggplot for forest change across 10 countries
forest_change_plot <- ggplot(forest_change, aes(x = year, y = forest_area, group = country)) +
  geom_line(aes(color = line_color), size = 1.2) +
  geom_point(aes(color = line_color), size = 3) +
  scale_color_identity() +
  geom_text(data = forest_change |> filter(year == "2020"), 
            aes(label = country, x = year, y = forest_area, color = line_color),
            hjust = -0.2, size = 3) +
  ggtitle("Forest Area Change (2000 vs 2020) Across Countries") +
  xlab("Year") +
  ylab("Forest Area (ha)") +
  theme_minimal()

# Display the plot
print(forest_change_plot)


```




```{r}
#visualize geographical distribution with log normalized data
country_density_map_logscale <- country_density_map|>
  mutate(
    log_count = log1p(individualCount)  # log1p handles zero counts
  )

global_size_range <- range(country_density_map_logscale$log_count, na.rm = TRUE)

# Iterate through each bird order and create maps for each country and year
bird_orders <- unique(country_density_map_logscale$order)

# Create a list to store plots
plots1 <- list()
c = 0
for (order1 in bird_orders) {
  for (country1 in countries) {
    for (year1 in years) {
      c <- c + 1
      # Filter data for the specific bird order, year, and country
      specific_data <- country_density_map_logscale |>
        filter(order == order1, year == year1, country == country1)

      # Join country map with bird observation data
      country_map <- country_maps |> filter(name_long == country1)

      # Create the plot
      plots1[[c]] <-
        ggplot() +
        geom_sf(data = country_map, fill = "gray90", color = "black") +
        geom_point(data = specific_data,
                   aes(x = decimalLongitude, y = decimalLatitude, size = individualCount),
                   alpha = 0.7) +
        scale_size_continuous(name = "Count", range = c(1, 10), limits = global_size_range) +
        ggtitle(paste("Bird Order:", order1, "Year:", year1, "Country:", country1)) +
        theme_bw()

      ggsave(paste("plot_geodistribution_logscale/bird_order:", order1, "year:", year1, "country:", country1,".png"), plots1[[c]], width = 10, height = 7)

    }
  }
}

```

```{r}
plots1[[1]]
plots1[[2]]
plots1[[3]]
```

```{r}
# # Merge aggregated counts back with deforestation data
model_data <- joined_data1 %>%
  select(country, year, iso, stable, loss, gain, disturb, net, change, gfw_area__ha) %>%
  distinct() %>%
  inner_join(order_counts, by = c("country", "year"))

#log normalizes the counts to take into account discrepancies in effort put into observation between 2000,2010  and 2020
model_data_log <- model_data|>
  mutate(
    log_count = log1p(order_count)  # log1p handles zero counts
  )
model_data_log
```

```{r}
model_data_log_normalized <- model_data_log |> 
  mutate(
    scaled_log_count = (log_count - min(log_count, na.rm = TRUE)) / 
                       (max(log_count, na.rm = TRUE) - min(log_count, na.rm = TRUE))
  )

model_data_log_normalized
```


```{r}
model_data_log_normalized_bins <- model_data_log_normalized |>
  mutate(
    scaled_log_count_bins = cut(
      scaled_log_count,
      breaks = seq(0, 1, length.out = 11),  # 10 intervals
      labels = paste0("b", 1:10),       # Create labels for the bins
      include.lowest = TRUE
    )
  )
model_data_log_normalized_bins
```

```{r}
model_data_log_normalized_bins|> group_by(country) |> 
  summarize(year)
```

```{r}
set.seed(12)  # For reproducibility
library(caret)

# Perform an initial split
split_index_bins <- initial_split(model_data_log_normalized_bins, prop = 0.8)

# Extract training and test sets
train_data_bins <- training(split_index_bins)
test_data_bins <- testing(split_index_bins)

set.seed(23)
# Perform an initial split
split_index <- initial_split(model_data_log_normalized, prop = 0.8)

# Extract training and test sets
train_data <- training(split_index)
test_data <- testing(split_index)

```


```{r}
library(e1071)

svm_recipe <-
  recipe(scaled_log_count_bins ~ stable + loss + gain + net + change + gfw_area__ha + order + year + country, 
         data = model_data_log_normalized_bins) |>
  step_mutate(country = as.factor(country)) |>
  step_dummy(country, one_hot = TRUE) |>
  step_mutate(order = as.factor(order)) |>
  step_dummy(order, one_hot = TRUE) |>
  step_mutate(year = factor(year, levels = c(2000, 2010, 2020)))


svm_pol <- svm_poly(cost = tune(), degree= tune()) |>
  set_engine("kernlab") |>
  set_mode("classification")

svm_pol_wflow <- workflow() |>
  add_model(svm_pol) |>
  add_recipe(svm_recipe)

folds_pol <- vfold_cv(train_data_bins, v = 4)

# the tuned parameters also have default values you can use
grid_pol <- grid_regular(cost(), degree(c(1,5)), levels = 5)

svm_pol_tune <- 
  svm_pol_wflow |>
  tune_grid(resamples = folds_pol, grid = grid_pol)


svm_metrics_pol <- collect_metrics(svm_pol_tune)
accuracy_results_pol <- svm_metrics_pol |> 
  filter(.metric == "accuracy")
print(accuracy_results_pol)

```

```{r}

library(tidyr)
library(ggplot2)

# Step 3: Create a Confusion Matrix with Explicit Levels
confusion_data <- svm_predictions |>
  conf_mat(truth = scaled_log_count_bins, estimate = .pred_class)

# Extract the confusion matrix counts as a data frame
confusion_data_df <- as_tibble(confusion_data$table) |>
  complete(Truth = paste0("b", 1:10), Prediction = paste0("b", 1:10), fill = list(n = 0))

# Step 4: Plot the Heatmap
heatmap_plot <- ggplot(confusion_data_df, aes(x = Prediction, y = Truth, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(
    title = "Confusion Matrix Heatmap",
    x = "Predicted Bin",
    y = "Actual Bin",
    fill = "Count"
  ) +
  theme_minimal()

print(heatmap_plot)


```

```{r}
# Check prediction distribution
svm_predictions |>
  count(.pred_class) |>
  arrange(desc(n)) |>
  mutate(proportion = n / sum(n))

# Compare predicted vs actual class distribution
predicted_distribution <- svm_predictions |>
  count(.pred_class) |>
  rename(Predicted = n)

actual_distribution <- test_data_bins |>
  count(scaled_log_count_bins) |>
  rename(Actual = n)

comparison <- full_join(predicted_distribution, actual_distribution, 
                        by = c(".pred_class" = "scaled_log_count_bins")) |>
  replace_na(list(Predicted = 0, Actual = 0))

print(comparison)
```

```{r}
#SVM RBF

svm_rbf <- svm_rbf(cost = tune(), rbf_sigma= tune()) |>
  set_engine("kernlab") |>
  set_mode("classification")

svm_rbf_wflow <- workflow() |>
  add_model(svm_rbf) |>
  add_recipe(svm_recipe)

folds_rbf <- vfold_cv(train_data_bins, v = 4)

# the tuned parameters also have default values you can use
grid_rbf <- grid_regular(cost(), rbf_sigma(), levels = 5)

svm_rbf_tune <- 
  svm_rbf_wflow |>
  tune_grid(resamples = folds_rbf, grid = grid_rbf)

svm_metrics_rbf <- collect_metrics(svm_rbf_tune)
accuracy_results_rbf <- svm_metrics_rbf |> 
  filter(.metric == "accuracy")
print(accuracy_results_rbf)

# Step 3: Create a Confusion Matrix with Explicit Levels
confusion_data <- svm_predictions |>
  conf_mat(truth = scaled_log_count_bins, estimate = .pred_class)

# Extract the confusion matrix counts as a data frame
confusion_data_df <- as_tibble(confusion_data$table) |>
  complete(Truth = paste0("b", 1:10), Prediction = paste0("b", 1:10), fill = list(n = 0))

# Step 4: Plot the Heatmap
heatmap_plot <- ggplot(confusion_data_df, aes(x = Prediction, y = Truth, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(
    title = "Confusion Matrix Heatmap",
    x = "Predicted Bin",
    y = "Actual Bin",
    fill = "Count"
  ) +
  theme_minimal()

print(heatmap_plot)

```



```{r}
model_data_log_normalized <- model_data_log_normalized |> 
  mutate(
    country = as.factor(country),
    order = as.factor(order),
    year = factor(year, levels = c(2000, 2010, 2020))
  )

# Step 2: Define Recipe
rf_recipe <- recipe(scaled_log_count ~ stable + loss + gain + net + change + gfw_area__ha + order + year + country, 
                    data = train_data) |>
  step_dummy(all_nominal(), -all_outcomes())  # Dummy-encode categorical variables

# Step 3: Define Model and Workflow
rf_model <- rand_forest(mtry = 10, trees = 500) |>  # Define Random Forest model
  set_engine("ranger", importance = "permutation") |>  # Set engine with permutation importance
  set_mode("regression")  # Set mode to regression

rf_wflow <- workflow() |>
  add_model(rf_model) |>
  add_recipe(rf_recipe)

# Step 4: Fit Model
rf_fit <- rf_wflow |>
  fit(data = train_data)

# Step 5: Predict on Test Data
rf_preds <- predict(rf_fit, new_data = test_data) |>
  bind_cols(test_data)

# Step 6: Evaluate Model Performance (MSE)
rf_metrics <- rf_preds |>
  metrics(truth = scaled_log_count, estimate = .pred)

# Extract MSE
rf_mse <- rf_metrics |>
  filter(.metric == "rmse") |>  # You can replace this with "mse" if you need it
  mutate(mse = .estimate^2) |>  # Convert RMSE to MSE
  pull(mse)

print(rf_mse)

```

```{r}
library(vip)
vip(rf_fit)
```
I realize that the year 2020 is very predictive, and the country tanzania very predictive, as both variables have distinctive natures appearantly. The variable that exists for all entries that is the most important is "net". Net change in forest size is somewhat indicative of predicting count.


```{r}
# Split Data
set.seed(123)  # Ensure reproducibility

# Step 2: Define Recipe
rf_recipe1 <- recipe(scaled_log_count_bins ~ stable + loss + gain + net + change + gfw_area__ha + order + year + country, 
                    data = train_data_bins) |>
  step_dummy(all_nominal(), -all_outcomes())  # Dummy-encode categorical variables

# Step 3: Define Model and Workflow
rf_model1 <- rand_forest(mtry = 10, trees = 500) |>  # Define Random Forest model
  set_engine("ranger", importance = "permutation") |>  # Set engine with permutation importance
  set_mode("classification")  # Set mode to regression

rf_wflow1 <- workflow() |>
  add_model(rf_model1) |>
  add_recipe(rf_recipe1)

# Step 4: Fit Model
rf_fit1 <- rf_wflow1 |>
  fit(data = train_data_bins)

# Step 5: Predict on Train Data
rf_preds1 <- predict(rf_fit1, new_data = train_data_bins) |>
  bind_cols(train_data_bins)

# Step 6: Evaluate Model Performance (MSE)
rf_metrics1 <- rf_preds1 |>
  metrics(truth = scaled_log_count_bins, estimate = .pred_class)

# Extract MSE
rf_mse <- rf_metrics1 |>
  filter(.metric == "rmse") |>  # You can replace this with "mse" if you need it
  pull(.estimate)

rf_mse

# Step 5: Predict on Test Data
rf_preds2 <- predict(rf_fit1, new_data = test_data_bins) |>
  bind_cols(test_data_bins)

# Step 6: Evaluate Model Performance (MSE)
rf_metrics2 <- rf_preds2 |>
  metrics(truth = scaled_log_count_bins, estimate = .pred_class)

# Extract MSE
rf_mse2 <- rf_metrics2 |>
  filter(.metric == "rmse") |>  # You can replace this with "mse" if you need it
  pull(.estimate)

```

```{r}
library(vip)
vip(rf_fit1)
```

```{r}
library(ggplot2)
library(tidyr)

# Step 6: Generate Confusion Matrix Data
conf_matrix <- rf_preds1 |>
  conf_mat(truth = scaled_log_count_bins, estimate = .pred_class)

# Convert Confusion Matrix to Tibble for Plotting
conf_matrix_tibble <- as_tibble(conf_matrix$table) |>
  complete(Truth = paste0("b", 1:10), Prediction = paste0("b", 1:10), fill = list(n = 0))

# Step 7: Heatmap of Predictions vs. Actuals
ggplot(conf_matrix_tibble, aes(x = Prediction, y = Truth, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Prediction vs. Actual Heatmap Train Data",
    x = "Predicted Bin",
    y = "Actual Bin",
    fill = "Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# Step 6: Generate Confusion Matrix Data
conf_matrix <- rf_preds2 |>
  conf_mat(truth = scaled_log_count_bins, estimate = .pred_class)

# Convert Confusion Matrix to Tibble for Plotting
conf_matrix_tibble <- as_tibble(conf_matrix$table) |>
  complete(Truth = paste0("b", 1:10), Prediction = paste0("b", 1:10), fill = list(n = 0))

# Step 7: Heatmap of Predictions vs. Actuals
ggplot(conf_matrix_tibble, aes(x = Prediction, y = Truth, fill = n)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Prediction vs. Actual Heatmap Test Data",
    x = "Predicted Bin",
    y = "Actual Bin",
    fill = "Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



```{r}
# KNN model with tuning
knn_recipe <- recipe(scaled_log_count_bins ~ stable + loss + gain + net + change + gfw_area__ha + year, 
                    data = train_data_bins)

knn_model <- nearest_neighbor() |>
  set_engine("kknn") |>
  set_mode("classification")


knn_wflow <- workflow() |>
  add_model(knn_model) |>
  add_recipe(knn_recipe)

fit_knn <- knn_wflow |> fit(data = train_data_bins)

set.seed(470)
knn_vfold <- vfold_cv(train_data_bins,
                          v = 5, strata = scaled_log_count_bins)
k_grid <- data.frame(neighbors = c(1,3,5))

knn_tune <- nearest_neighbor(neighbors = tune()) |>
  set_engine("kknn") |>
  set_mode("classification")

knn_wflow_tune <- workflow() |>
  add_model(knn_model) |>
  add_recipe(knn_recipe)
```

```{r}
knn_wflow_tune |>
  tune_grid(resamples = knn_vfold, 
           grid = k_grid) |>
  collect_metrics() |>
  filter(.metric == "accuracy")

```



